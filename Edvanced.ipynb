{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a7ab79-d990-4cf7-9b91-1e1a438d1721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de26c09f-e96d-4c8b-bd9c-a75316c9b4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13fa4544-d7be-4c34-a469-d14639de79f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfadcb9c-c146-42c0-9b5d-94b508d0abce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb247b10-f167-40e6-ac72-1726a86e2823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file preprocessor_config.json from cache at C:\\Users\\mahmoud\\.cache\\huggingface\\hub\\models--microsoft--trocr-small-handwritten\\snapshots\\b4648cfa171985a6745f37ddd637e98c0da958ac\\preprocessor_config.json\n",
      "size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}, {'longest_edge'}, {'max_width', 'max_height'}), got 384. Converted to {'height': 384, 'width': 384}.\n",
      "crop_size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}, {'longest_edge'}, {'max_width', 'max_height'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
      "Image processor DeiTImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  },\n",
      "  \"do_center_crop\": false,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"DeiTImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 384,\n",
      "    \"width\": 384\n",
      "  }\n",
      "}\n",
      "\n",
      "loading file sentencepiece.bpe.model from cache at C:\\Users\\mahmoud\\.cache\\huggingface\\hub\\models--microsoft--trocr-small-handwritten\\snapshots\\b4648cfa171985a6745f37ddd637e98c0da958ac\\sentencepiece.bpe.model\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\mahmoud\\.cache\\huggingface\\hub\\models--microsoft--trocr-small-handwritten\\snapshots\\b4648cfa171985a6745f37ddd637e98c0da958ac\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\mahmoud\\.cache\\huggingface\\hub\\models--microsoft--trocr-small-handwritten\\snapshots\\b4648cfa171985a6745f37ddd637e98c0da958ac\\tokenizer_config.json\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file chat_template.jinja from cache at None\n",
      "Processor TrOCRProcessor:\n",
      "- image_processor: DeiTImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  },\n",
      "  \"do_center_crop\": false,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"DeiTImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 384,\n",
      "    \"width\": 384\n",
      "  }\n",
      "}\n",
      "\n",
      "- tokenizer: XLMRobertaTokenizer(name_or_path='microsoft/trocr-small-handwritten', vocab_size=64002, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t64001: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
      "}\n",
      ")\n",
      "\n",
      "{\n",
      "  \"processor_class\": \"TrOCRProcessor\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223cc345fc65434487f232acd97a48dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mahmoud\\AppData\\Local\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mahmoud\\.cache\\huggingface\\hub\\models--microsoft--trocr-small-handwritten. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "loading configuration file config.json from cache at C:\\Users\\mahmoud\\.cache\\huggingface\\hub\\models--microsoft--trocr-small-handwritten\\snapshots\\b4648cfa171985a6745f37ddd637e98c0da958ac\\config.json\n",
      "Model config VisionEncoderDecoderConfig {\n",
      "  \"architectures\": [\n",
      "    \"VisionEncoderDecoderModel\"\n",
      "  ],\n",
      "  \"decoder\": {\n",
      "    \"activation_dropout\": 0.0,\n",
      "    \"activation_function\": \"relu\",\n",
      "    \"add_cross_attention\": true,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"classifier_dropout\": 0.0,\n",
      "    \"cross_attention_hidden_size\": 384,\n",
      "    \"d_model\": 256,\n",
      "    \"decoder_attention_heads\": 8,\n",
      "    \"decoder_ffn_dim\": 1024,\n",
      "    \"decoder_layerdrop\": 0.0,\n",
      "    \"decoder_layers\": 6,\n",
      "    \"dropout\": 0.1,\n",
      "    \"init_std\": 0.02,\n",
      "    \"is_decoder\": true,\n",
      "    \"layernorm_embedding\": true,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"model_type\": \"trocr\",\n",
      "    \"scale_embedding\": true,\n",
      "    \"tie_word_embeddings\": false,\n",
      "    \"use_cache\": false,\n",
      "    \"use_learned_position_embeddings\": true,\n",
      "    \"vocab_size\": 64044\n",
      "  },\n",
      "  \"encoder\": {\n",
      "    \"attention_probs_dropout_prob\": 0.0,\n",
      "    \"encoder_stride\": 16,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.0,\n",
      "    \"hidden_size\": 384,\n",
      "    \"image_size\": 384,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 1536,\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"model_type\": \"deit\",\n",
      "    \"num_attention_heads\": 6,\n",
      "    \"num_channels\": 3,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"patch_size\": 16,\n",
      "    \"pooler_act\": \"tanh\",\n",
      "    \"pooler_output_size\": 384,\n",
      "    \"qkv_bias\": true\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"model_type\": \"vision-encoder-decoder\",\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716a6668abd2463fb4b7e8ca9d780589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/246M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\mahmoud\\.cache\\huggingface\\hub\\models--microsoft--trocr-small-handwritten\\snapshots\\b4648cfa171985a6745f37ddd637e98c0da958ac\\pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Attempting to create safetensors variant\n",
      "Safetensors PR exists\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b68db394da66441db8172980122c25ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/246M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Instantiating DeiTModel model under default dtype torch.float32.\n",
      "Instantiating TrOCRForCausalLM model under default dtype torch.float32.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "Config of the encoder: <class 'transformers.models.deit.modeling_deit.DeiTModel'> is overwritten by shared encoder config: DeiTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 384,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"deit\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"pooler_act\": \"tanh\",\n",
      "  \"pooler_output_size\": 384,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 384,\n",
      "  \"d_model\": 256,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 1024,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 64044\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing VisionEncoderDecoderModel.\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce1e7ecf97e241adada6ea49a2b9d8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file generation_config.json from cache at C:\\Users\\mahmoud\\.cache\\huggingface\\hub\\models--microsoft--trocr-small-handwritten\\snapshots\\b4648cfa171985a6745f37ddd637e98c0da958ac\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load model & processor\n",
    "#use base instead of small if bad accuracy irl\n",
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten',use_fast=False)\n",
    "model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb97ce73-07ab-4fc4-9f51-0436b06561aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d8f0f983-1922-4d78-b0d8-323d907e5ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BubbleSheetProcessor:\n",
    "    def __init__(self, image_path):\n",
    "        self.image_path = image_path\n",
    "        self.nc = None\n",
    "        self.image = None\n",
    "        self.gray = None\n",
    "        self.warped = None\n",
    "        self.cropped = None\n",
    "        self.thresh = None\n",
    "        self.bubble_contours = None\n",
    "        self.answer_matrix = None\n",
    "        self.student_id=None\n",
    "        self.bubbles_area_x, self.bubbles_area_y, self.bubbles_area_w, self.bubbles_area_h=None,None,None,None\n",
    "        self.marker_area=None\n",
    "\n",
    "    def load_image(self):\n",
    "        self.image = cv2.imread(self.image_path)\n",
    "        self.gray = cv2.cvtColor(self.image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    def get_id_corner_map(self):\n",
    "        aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_100)\n",
    "        parameters = cv2.aruco.DetectorParameters()\n",
    "        detector = cv2.aruco.ArucoDetector(aruco_dict, parameters)\n",
    "\n",
    "        corners, ids, _ = detector.detectMarkers(self.gray)\n",
    "        if ids is None or len(ids) < 4:\n",
    "            raise ValueError(\"Not all 4 ArUco markers were detected.\")\n",
    "\n",
    "        id_corner_map = {id[0]: corner for id, corner in zip(ids, corners)}\n",
    "\n",
    "        if 48 in id_corner_map:\n",
    "            self.nc = 5\n",
    "        elif 49 in id_corner_map:\n",
    "            self.nc = 4\n",
    "        else:\n",
    "            raise ValueError(\"Required ArUco marker not found.\")\n",
    "        #give the area of a circle with diameter equals to the edge of a marker\n",
    "        marker_arr=id_corner_map[30]\n",
    "        self.marker_area=np.pi*(int(abs(marker_arr[0][1][0]-marker_arr[0][0][0]))/2)**2\n",
    "        return id_corner_map\n",
    "\n",
    "    def get_center(self, corner):\n",
    "        return corner[0].mean(axis=0)\n",
    "\n",
    "    def get_centroid(self, array):\n",
    "        return np.array(array).mean(axis=0)\n",
    "\n",
    "    def get_roi(self, id_corner_map):\n",
    "        if 48 in id_corner_map:\n",
    "            ordered_ids = [30, 10, 48, 34]  # TL, TR, BR, BL\n",
    "        else:  # 49 is guaranteed if 48 isn't\n",
    "            ordered_ids = [30, 10, 49, 34]\n",
    "\n",
    "        ordered_pts = [self.get_center(id_corner_map[id]) for id in ordered_ids]\n",
    "        return np.array(ordered_pts, dtype='float32')\n",
    "\n",
    "    def get_warped_image(self, src_pts):\n",
    "        width, height = int(self.image.shape[0] * 0.764), self.image.shape[0]\n",
    "        dst_pts = np.array([\n",
    "            [0, 0],\n",
    "            [width - 1, 0],\n",
    "            [width - 1, height - 1],\n",
    "            [0, height - 1]\n",
    "        ], dtype='float32')\n",
    "\n",
    "        M = cv2.getPerspectiveTransform(src_pts, dst_pts)\n",
    "        return cv2.warpPerspective(self.image, M, (width, height))\n",
    "\n",
    "    def get_largest_contour(self, image, drawContour=False):\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        edged = cv2.Canny(gray, 30, 200)\n",
    "\n",
    "        contours, _ = cv2.findContours(edged, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        if drawContour:\n",
    "            cv2.drawContours(image, largest_contour, -1, (0, 255, 0), 3)\n",
    "            cv2.imshow('largest contour', image)\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "        return largest_contour\n",
    "\n",
    "    def warp_paper(self):\n",
    "        id_corner_map = self.get_id_corner_map()\n",
    "        roi = self.get_roi(id_corner_map)\n",
    "        self.warped = self.get_warped_image(roi)\n",
    "\n",
    "    def preprocess_cropped(self):\n",
    "        self.bubbles_area_x, self.bubbles_area_y, self.bubbles_area_w, self.bubbles_area_h = cv2.boundingRect(self.get_largest_contour(self.warped))\n",
    "        self.cropped = self.warped[self.bubbles_area_y+10:self.bubbles_area_y+self.bubbles_area_h-10, \n",
    "                                    self.bubbles_area_x+10:self.bubbles_area_x+self.bubbles_area_w-10]\n",
    "        \n",
    "        cropped_gray = cv2.cvtColor(self.cropped, cv2.COLOR_BGR2GRAY)\n",
    "        blurred = cv2.GaussianBlur(cropped_gray, (3, 3), 0)\n",
    "\n",
    "        self.thresh = cv2.adaptiveThreshold(\n",
    "            blurred, 255, \n",
    "            cv2.ADAPTIVE_THRESH_MEAN_C, \n",
    "            cv2.THRESH_BINARY_INV, \n",
    "            25, 4\n",
    "        )\n",
    "\n",
    "    def extract_bubble_contours(self, drawContours=False):\n",
    "        contours, _ = cv2.findContours(self.thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "        bubble_contours = []\n",
    "\n",
    "        for c in contours:\n",
    "            area = cv2.contourArea(c)\n",
    "            x, y, w, h = cv2.boundingRect(c)\n",
    "            aspect_ratio = w / float(h)\n",
    "\n",
    "            if area > self.marker_area and 0.9 < aspect_ratio < 1.1:\n",
    "                bubble_contours.append(c)\n",
    "\n",
    "        bubble_contours.sort(key=lambda b: self.get_centroid(b)[0][1])\n",
    "\n",
    "        for i in range(10):  # 10 rows expected\n",
    "            start = i * self.nc * 3\n",
    "            end = (i + 1) * self.nc * 3\n",
    "            bubble_contours[start:end] = sorted(\n",
    "                bubble_contours[start:end],\n",
    "                key=lambda b: self.get_centroid(b)[0][0]\n",
    "            )\n",
    "\n",
    "        if drawContours:\n",
    "            output = self.cropped.copy()\n",
    "            cv2.drawContours(output, bubble_contours, -1, (0, 255, 0), 2)\n",
    "            output_resized = cv2.resize(output, (800, int(output.shape[0] * 800 / output.shape[1])))\n",
    "            cv2.imshow('Detected Bubbles', output_resized)\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "        self.bubble_contours = bubble_contours\n",
    "\n",
    "    def get_filled_bubbles_matrix(self):\n",
    "        extracted = []\n",
    "\n",
    "        for c in self.bubble_contours:\n",
    "            mask = np.zeros(self.thresh.shape, dtype=\"uint8\")\n",
    "            cv2.drawContours(mask, [c], -1, 255, -1)\n",
    "            masked = cv2.bitwise_and(self.thresh, self.thresh, mask=mask)\n",
    "\n",
    "            total = cv2.countNonZero(mask)\n",
    "            filled = cv2.countNonZero(masked)\n",
    "            ratio = filled / total\n",
    "\n",
    "            extracted.append(1 if ratio > 0.5 else 0)\n",
    "\n",
    "        self.answer_matrix = np.array(extracted).reshape(-1, self.nc)\n",
    "    def detect_student_id(self):\n",
    "        id_area=self.warped[0:self.bubbles_area_y, int(self.warped.shape[1]*.75):]\n",
    "        x, y, w, h = cv2.boundingRect(self.get_largest_contour(id_area))\n",
    "        \n",
    "        student_id_img=id_area[y+5:y+h-5, x+5:x+w-5]\n",
    "        \n",
    "        pixel_values = processor(images=student_id_img, return_tensors=\"pt\").pixel_values\n",
    "        # Run inference\n",
    "        generated_ids = model.generate(pixel_values,max_length=14,min_length=4,num_beams =2,early_stopping =True)\n",
    "        self.student_id = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "        \n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        self.load_image()\n",
    "        self.warp_paper()\n",
    "        self.preprocess_cropped()\n",
    "        self.extract_bubble_contours(True)\n",
    "        self.get_filled_bubbles_matrix()\n",
    "        self.detect_student_id()\n",
    "        return self.student_id, self.answer_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df37519f-51f3-4ddc-9087-429f861a6b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "123195b8-f73d-4b46-aaeb-c77ec0cdc76b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 150 into shape (4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m s\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      3\u001b[0m sheet_processor\u001b[38;5;241m=\u001b[39mBubbleSheetProcessor(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDownloads/pi.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(sheet_processor\u001b[38;5;241m.\u001b[39mrun_pipeline())\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39ms)\n",
      "Cell \u001b[1;32mIn[64], line 169\u001b[0m, in \u001b[0;36mBubbleSheetProcessor.run_pipeline\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_cropped()\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_bubble_contours(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_filled_bubbles_matrix()\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetect_student_id()\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudent_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manswer_matrix\n",
      "Cell \u001b[1;32mIn[64], line 150\u001b[0m, in \u001b[0;36mBubbleSheetProcessor.get_filled_bubbles_matrix\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    146\u001b[0m     ratio \u001b[38;5;241m=\u001b[39m filled \u001b[38;5;241m/\u001b[39m total\n\u001b[0;32m    148\u001b[0m     extracted\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ratio \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manswer_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(extracted)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnc)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 150 into shape (4)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "s=time.time()\n",
    "sheet_processor=BubbleSheetProcessor('Downloads/pi.jpg')\n",
    "print(sheet_processor.run_pipeline())\n",
    "print(time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "77fd2ad6-0045-405d-b64c-63d89a38de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= np.array([[[1199., 1574.],\n",
    "        [1245., 1574.],\n",
    "        [1244., 1620.],\n",
    "        [1199., 1620.]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "58293b0e-e535-45db-a35a-a74dfe99945f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1661.9025137490005"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe4ba6c-c608-4606-b025-49af7c14c9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(model_answer_image, *students_answers_images):\n",
    "    processor=BubbleSheetProcessor(model_answer_image)\n",
    "    _,mam=processor.run_pipeline()\n",
    "    \n",
    "    for student_answer in students_answers_images:\n",
    "        processor=BubbleSheetProcessor(student_answer)\n",
    "        student_id,sam=processor.run_pipeline()\n",
    "        sam==mam\n",
    "        print(student_id,np.all(sam==mam,axis=1).sum())\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275460ce-07c7-4a88-9ce7-6c9604a19b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results('Downloads/modelanswer.jpg','Downloads/e.jpg','Downloads/pi.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202597d1-05e4-404e-ae6d-2ac6cd5c0616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e279bfb-d708-49e2-a803-aeb66a945be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1=np.array([[1, 1, 0, 0, 0],\n",
    "       [1, 0, 0, 0, 0],\n",
    "       [1, 0, 0, 0, 0],\n",
    "       [0, 1, 0, 0, 0],\n",
    "       [0, 1, 0, 0, 0],\n",
    "       [0, 1, 0, 0, 0],\n",
    "       [0, 0, 1, 0, 0],\n",
    "       [0, 0, 1, 0, 0],\n",
    "       [0, 0, 1, 0, 0],\n",
    "       [0, 0, 0, 1, 0],\n",
    "       [0, 0, 0, 1, 0],\n",
    "       [0, 0, 0, 1, 0],\n",
    "       [0, 0, 0, 0, 1],\n",
    "       [0, 0, 0, 0, 1],\n",
    "       [0, 0, 0, 0, 1],\n",
    "       [0, 0, 0, 0, 1],\n",
    "       [0, 0, 0, 0, 1],\n",
    "       [0, 0, 0, 0, 1],\n",
    "       [0, 0, 0, 1, 0],\n",
    "       [0, 0, 0, 1, 0],\n",
    "       [0, 0, 0, 1, 0],\n",
    "       [0, 0, 1, 0, 0],\n",
    "       [0, 0, 0, 0, 0],\n",
    "       [0, 0, 1, 0, 0],\n",
    "       [0, 1, 0, 0, 0],\n",
    "       [0, 1, 0, 0, 0],\n",
    "       [0, 1, 0, 0, 0],\n",
    "       [1, 0, 0, 0, 0],\n",
    "       [1, 0, 0, 0, 0],\n",
    "       [1, 0, 0, 0, 0]])\n",
    "\n",
    "\n",
    "s2=np.array([[1, 1, 0, 0, 0],\n",
    "       [0, 1, 0, 0, 0],\n",
    "       [0,1, 0, 0, 0],\n",
    "       [0, 1, 0, 0, 0],\n",
    "       [0, 1, 0, 0, 0],\n",
    "       [0, 1, 0, 0, 0],\n",
    "       [0, 0, 1, 0, 0],\n",
    "       [0, 0, 1, 0, 0],\n",
    "       [0, 0, 1, 0, 0],\n",
    "       [0, 0, 0, 1, 0],\n",
    "       [0, 0, 0, 1, 0],\n",
    "       [0, 0, 0, 1, 0],\n",
    "       [0, 0, 0, 0, 1],\n",
    "       [0, 0, 0, 0, 1],\n",
    "       [0, 0, 0, 0, 1],\n",
    "       [0, 0, 0, 0, 1],\n",
    "       [0, 0, 0, 0, 1],\n",
    "       [0, 0, 0, 0, 1],\n",
    "       [0, 0, 0, 1, 0],\n",
    "       [0, 0, 0, 1, 0],\n",
    "       [0, 0, 0, 1, 0],\n",
    "       [0, 0, 1, 0, 0],\n",
    "       [0, 0, 0, 0, 0],\n",
    "       [0, 0, 1, 0, 0],\n",
    "       [0, 1, 0, 0, 0],\n",
    "       [0, 1, 0, 0, 0],\n",
    "       [0, 1, 0, 0, 0],\n",
    "       [1, 0, 0, 0, 0],\n",
    "       [1, 0, 0, 0, 0],\n",
    "       [1, 0, 0, 0, 0]])\n",
    "\n",
    "s3=np.array([[1, 0, 0, 0, 0],\n",
    "       [1, 0, 0, 0, 0],\n",
    "       [1, 0, 0, 0, 0],\n",
    "       [0, 1, 0, 0, 0],\n",
    "       [0, 1, 0, 0, 0],\n",
    "       [0, 1, 0, 0, 0],\n",
    "       [0, 0, 1, 0, 0],\n",
    "       [0, 0, 0, 1, 0],\n",
    "       [0, 0, 1, 0, 0],\n",
    "       [0, 0, 0, 1, 0],\n",
    "       [0, 0, 0, 1, 0],\n",
    "       [0, 0, 1, 0, 0],\n",
    "       [0, 0, 0, 0, 1],\n",
    "       [0, 0, 0, 0, 1],\n",
    "       [0, 0, 0, 0, 1],\n",
    "       [1, 0, 0, 0, 0],\n",
    "       [1, 0, 0, 0, 0],\n",
    "       [1, 0, 0, 0, 0],\n",
    "       [0, 1, 0, 0, 0],\n",
    "       [0, 1, 0, 0, 0],\n",
    "       [0, 0, 1, 0, 0],\n",
    "       [0, 0, 1, 0, 0],\n",
    "       [0, 0, 1, 0, 0],\n",
    "       [0, 0, 1, 0, 0],\n",
    "       [0, 0, 0, 1, 0],\n",
    "       [0, 0, 0, 1, 0],\n",
    "       [0, 0, 0, 1, 0],\n",
    "       [0, 0, 0, 0, 1],\n",
    "       [0, 0, 0, 0, 1],\n",
    "       [0, 0, 1, 0, 0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9ea84f89-9589-49a9-86f9-d0eefad37136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(s1^s2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "64a0ca76-a00f-4a0b-ae80-4f3f1a67d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=np.array([[1, 0, 0, 0, 0],\n",
    "       [1, 0, 0, 0, 0],\n",
    "       [1, 0, 0, 0, 0],\n",
    "       [0, 1, 0, 0, 0],\n",
    "       [0, 1, 0, 0, 0],\n",
    "       [0, 1, 0, 0, 0],\n",
    "       [0, 0, 1, 0, 0],\n",
    "       [0, 0, 1, 0, 0],\n",
    "       [0, 0, 1, 0, 0],\n",
    "       [0, 0, 0, 1, 0],\n",
    "       [0, 0, 0, 1, 0],\n",
    "       [0, 0, 0, 1, 0],\n",
    "       [0, 0, 0, 0, 1],\n",
    "       [0, 0, 0, 0, 1],\n",
    "       [0, 0, 0, 0, 1],\n",
    "       [1, 0, 0, 0, 0],\n",
    "       [1, 0, 0, 0, 0],\n",
    "       [1, 0, 0, 0, 0],\n",
    "       [0, 1, 0, 0, 0],\n",
    "       [0, 1, 0, 0, 0],\n",
    "       [0, 1, 0, 0, 0],\n",
    "       [0, 0, 1, 0, 0],\n",
    "       [0, 0, 1, 0, 0],\n",
    "       [0, 0, 1, 0, 0],\n",
    "       [0, 0, 0, 1, 0],\n",
    "       [0, 0, 0, 1, 0],\n",
    "       [0, 0, 0, 1, 0],\n",
    "       [0, 0, 0, 0, 1],\n",
    "       [0, 0, 0, 0, 1],\n",
    "       [0, 0, 0, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b107f-c2d9-497d-bf09-3880303a6d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b195d5-7e47-4b7e-8d4d-7a2f1d303070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b736c9d4-4d8b-43b1-b4d3-d0d538f77bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
